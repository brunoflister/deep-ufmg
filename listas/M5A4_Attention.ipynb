{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S05A04_1_Seq2Seq_Attention_(exercicio_attention).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tCy34_fj_E8"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports básicos\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch==1.7.1\n",
        "! pip install torchtext==0.8.1\n",
        "! pip install torchvision==0.8.2"
      ],
      "metadata": {
        "id": "oF61oBHUJCu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeSdMJtajy_z"
      },
      "source": [
        "! pip install unidecode\n",
        "\n",
        "# Basic imports.\n",
        "import os\n",
        "import unidecode, re\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "import spacy\n",
        "! python -m spacy download en\n",
        "! python -m spacy download fr\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQWXOc1GkDTQ"
      },
      "source": [
        "# Setting predefined arguments.\n",
        "args = {\n",
        "    'epoch_num': 100,       # Number of epochs.\n",
        "    'lr': 1e-3,           # Learning rate.\n",
        "    'weight_decay': 5e-4, # L2 penalty.\n",
        "    'momentum': 0.9,      # Momentum.\n",
        "    'num_workers': 6,     # Number of workers on data loader.\n",
        "    'batch_size': 10,     # Mini-batch size.\n",
        "    'max_length': 50,    # Maximun length of predicted sentence\n",
        "}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    args['device'] = torch.device('cuda')\n",
        "else:\n",
        "    args['device'] = torch.device('cpu')\n",
        "\n",
        "print(args['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHsYj_w9H6TL"
      },
      "source": [
        "# Generating Sequences\n",
        "\n",
        "\n",
        "Dentre os tipos de problemas solucionáveis com modelos recorrentes, dois deles são baseados em geração de sequências: Os problemas One-to-Many,  e os Many-to-Many não sincronizados. <br>\n",
        "\n",
        "Tipicamente os modelos de geração de sequências são baseados em arquiteturas **Encoder-Decoder**, onde a entrada é codificada para uma forma fixa, e então decodificada passo a passo em uma sequência.\n",
        "\n",
        "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGMQRCmguT4A"
      },
      "source": [
        "## Sequence-to-sequence models (Seq2Seq)\n",
        "\n",
        "Modelos Sequence-to-Sequence (Seq2Seq) partem do mesmo princípio do Image Captioning, porém a entrada também é sequencial, de modo que a codificação também é realizada por um modelo recorrente. \n",
        "\n",
        "A atividade de hoje é no contexto de Neural Machine Translation (NMT), cujo pipeline é representado de forma simplificada a seguir. Note que o idioma source (francês) necessita apenas do token de finalização de sentença, enquanto o idioma target precisa de ambos os inicializadores e os finalizadores (```<sos>```, ```<eos>```), visto que a entrada da rede precisa do token de inicialização, mas a saída, através da qual será calculada a loss, é produzida apenas com a finalização.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "Imagem retirada do tutorial de NMT do Pytorch: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "Encontre modelos de linguagem pré-treinados e arquiteturas implementadas em: http://opennmt.net/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVIb6Cw3kEMr"
      },
      "source": [
        "# Baixando Dataset\n",
        "!wget https://www.dropbox.com/s/gq36ksk347d36ln/translation_data.zip\n",
        "!unzip translation_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ7knpeZs2fv"
      },
      "source": [
        "# Criando CSV de treino e teste para carregar com o TabularDataset\n",
        "\n",
        "translation_path = 'data/eng-fra.txt'\n",
        "\n",
        "samples = open(translation_path).read().split('\\n')\n",
        "  \n",
        "# Write txt to csv\n",
        "lines = (line.split(\"\\t\") for line in samples)\n",
        "with open('translation_data.csv', 'w') as out_file:\n",
        "    writer = csv.writer(out_file)\n",
        "    writer.writerow(('English', 'French'))\n",
        "    writer.writerows(lines)\n",
        "    \n",
        "df = pd.read_csv('translation_data.csv')\n",
        "\n",
        "# Reducing data (throwing out samples)\n",
        "train, _ = train_test_split(df, test_size=0.95)\n",
        "\n",
        "# Split train and test set \n",
        "train, test = train_test_split(train, test_size=0.02)\n",
        "\n",
        "train.to_csv('train.csv', index=False)\n",
        "test.to_csv('test.csv', index=False)\n",
        "\n",
        "\n",
        "df = pd.read_csv('test.csv')\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnMn_u9wNc5-"
      },
      "source": [
        "# Preparação do dataset:\n",
        "# Tokenização e inclusão dos tokens especiais (<eos>, <sos>, <pad>, <unk>)\n",
        "\n",
        "def normalize_string(sentence):\n",
        "  \n",
        "  new_sentence = []\n",
        "  for word in sentence:\n",
        "    s = unidecode.unidecode(word.lower())\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "\n",
        "    if s.strip() == '': continue \n",
        "\n",
        "    new_sentence.append(s.strip())\n",
        "\n",
        "  return new_sentence\n",
        "\n",
        "\n",
        "TEXT_FR = data.Field(tokenize = 'spacy', \n",
        "                     tokenizer_language='fr', \n",
        "                     preprocessing = normalize_string,\n",
        "                     include_lengths=True, \n",
        "                     eos_token = \"<eos>\")\n",
        "TEXT_EN = data.Field(tokenize = 'spacy', \n",
        "                     tokenizer_language='en', \n",
        "                     preprocessing = normalize_string,\n",
        "                     include_lengths=True, \n",
        "                     init_token = \"<sos>\", \n",
        "                     eos_token = \"<eos>\")\n",
        "\n",
        "fields = [('text_en', TEXT_EN), ('text_fr', TEXT_FR)]\n",
        "train_data, test_data = data.TabularDataset.splits(\n",
        "                                  path = '.',\n",
        "                                  train = 'train.csv',\n",
        "                                  test = 'test.csv',\n",
        "                                  format = 'csv',\n",
        "                                  fields = fields,\n",
        "                                  skip_header = True)\n",
        "\n",
        "for sample in train_data:\n",
        "  print(sample.text_fr)\n",
        "  print(sample.text_en)\n",
        "  break\n",
        "  \n",
        "print(len(train_data), len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzHUADPwPJLz"
      },
      "source": [
        "# Criando vocabulário\n",
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT_EN.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE)\n",
        "\n",
        "TEXT_FR.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE)\n",
        "\n",
        "\n",
        "# Instanciando bucket iterator\n",
        "# Note que a ordenação é definida pelo \n",
        "# comprimento do par ** (en, fr) **\n",
        "# Precisaremos empacotar ambas as sequências \n",
        "# para realizar o forward encapsulado.\n",
        "train_iterator = data.BucketIterator(\n",
        "    train_data, \n",
        "    batch_size = 1,\n",
        "    sort_key = lambda x:(len(x.text_fr), len(x.text_en)),\n",
        "    sort_within_batch = True,\n",
        "    device = args['device'])\n",
        "\n",
        "\n",
        "test_iterator = data.BucketIterator(\n",
        "    test_data, \n",
        "    batch_size = 1,\n",
        "    sort_within_batch = False,\n",
        "    device = args['device'])\n",
        "\n",
        "for k, batch in enumerate(train_iterator):\n",
        "  text_fr, lengths_fr = batch.text_fr\n",
        "  text_en, lengths_en = batch.text_en\n",
        "  \n",
        "  print(text_fr)\n",
        "  print(text_en)\n",
        "  print(lengths_fr)\n",
        "  print(lengths_en)\n",
        "  break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q94zieSNEKj"
      },
      "source": [
        "## Bahdanau Attention\n",
        "\n",
        "Tutorial com gifs animados do Towards Data Science: https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3\n",
        "\n",
        "Tutorial do Tensorflow usado como referência para a implementação: https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
        "\n",
        "![](https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg)\n",
        "\n",
        "### Passo a passo\n",
        "<img width=650 src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\">\n",
        "\n",
        "<img width=650 src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\">\n",
        "\n",
        "Pseudo-código:\n",
        "\n",
        "* `score = FC(tanh(FC(encoder_outputs) + FC(decoder_hidden)))`\n",
        "* `attention weights = softmax(score, axis = 0)`. \n",
        "   * Softmax é aplicada à dimensão 0 já que o score de alinhamento tem dimensionalidade `(seq_length, batch_size, hidden_size)`. `seq_length` é a quantidade de elementos da sequência. Já que buscamos associar um peso a cada elemento, devemos aplicar a softmax na dimensão correspondente.\n",
        "* `context vector = sum(attention weights * EO, axis = 0)`. \n",
        "* `embedding_output` = A entrada para o decoder (X) é passada por uma camada de embedding.\n",
        "* `merged_vector = concat(embedding output, context vector)`. O vetor com a entrada e o `context_vector` são alimentados para a GRU do decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzd-lVT-xUDV"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "  def __init__(self, encoder_size, decoder_size):\n",
        "\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    \n",
        "    self.W1 = ### TODO\n",
        "    self.W2 = ### TODO\n",
        "    self.V  = ### TODO # values\n",
        "\n",
        "  def forward(self, decoder_hidden, encoder_outputs): # query, keys\n",
        "    \n",
        "    # decoder_hidden   (1 x B x H)\n",
        "    # encoder_outputs  (S x B x H)\n",
        "\n",
        "    # score de alinhamento (S x B x 1)\n",
        "    score = ### TODO\n",
        "\n",
        "    # score como distribuição de probabilidades\n",
        "    # softmax aplicada na dimensão da sequência\n",
        "    # queremos um peso para cada elemento da sequência.\n",
        "    attention_weights = ### TODO\n",
        "\n",
        "    # aplica os scores (S x B x 1) no encoder_outputs (S x B x H)\n",
        "    context_vector = ### TODO\n",
        "\n",
        "    # Soma na dimensão da sequência -> (B x H)\n",
        "    context_vector = ### TODO\n",
        "    \n",
        "    return context_vector, attention_weights.squeeze(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSkInJB2lAs4"
      },
      "source": [
        "### EncoderRNN\n",
        "\n",
        "Implemente a classe **EncoderRNN** composta de um passo de representação de palavras e um passo de caracterização de sequência, ou seja, implemente as seguintes camadas:\n",
        "*  Embedding: como não usaremos vetores pré-treinados, a dimensão de saída dessa camada é um hiperparâmetro livre. Sugestão de tamanho: ```100```. Sua entrada é definida pelo tamanho do dicionário do idioma source (nesse caso o francês).\n",
        "*  Dropout: ```0.1``` <br><br>\n",
        "*  GRU: Defina ```hidden_size = 128``` para o encoder\n",
        "\n",
        "### DecoderRNN\n",
        "\n",
        "Implemente a classe **DecoderRNN**. Novamente é necessário uma camada de representação de palavras, seguida de uma camada de caracterização de sequências. Além disso, o decoder também deve possuir uma camada Linear de classificação, que transformará a representação de cada timestep (saída da RNN) em uma predição da próxima palavra.\n",
        "\n",
        "* Embedding: A entrada definida pelo vocabulário do idioma target (inglês), saída é um hiperparâmetro livre (sugestão: ```100```).\n",
        "* Dropout: ```0.1``` <br><br>\n",
        "* GRU: Seus hiperparâmetros são inferíveis a partir das outras informações. **Lembre-se que a inicialização do hidden state é dada pelo último hidden state do encoder** (veja na função train). <br><br>\n",
        "* Linear: Parâmetros inferíveis pelas outras informações. Quantas classes tem a predição de palavras em inglês?\n",
        "* LogSoftmax: ativação da classificação.\n",
        "\n",
        "No decoder, **implemente ambos os forward** para treinamento (encapsulado em batches) e para inferência (loop explícito sem targets).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j8aLVymyvhGtM0lpfON0aDyPvUf4700U\" width=\"850\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCV4MQ9WuaD8"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1, num_layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        ### TODO\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "      \n",
        "        ### TODO\n",
        "        \n",
        "        ## Por favor, mantenha o retorno dessa forma\n",
        "        return outputs, hidden[-1:]\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(args['device'])\n",
        "      \n",
        "      \n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, max_length):\n",
        "\n",
        "        ### TODO arquitetura do decoder\n",
        "        ## GRU's input is [context, embed] (2*hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, target=None, lengths=None):\n",
        "\n",
        "        if target is not None:\n",
        "          return self.teacher_forcing(hidden, encoder_outputs, target, lengths)\n",
        "\n",
        "        # token de input inicial\n",
        "        token = torch.tensor(TEXT_EN.vocab.stoi[\"<sos>\"]).unsqueeze(0).to(args['device'])\n",
        "        \n",
        "        outputs = []\n",
        "        att_list = []\n",
        "        for i in range(self.max_length):\n",
        "          \n",
        "          # Build context vector with the attention layer\n",
        "          ### TODO\n",
        "\n",
        "          # concatenate embedded representation with context vector\n",
        "          ### TODO\n",
        "\n",
        "          # recurrent forward and inference\n",
        "          ### TODO\n",
        "          \n",
        "          topv, topi = output.topk(1)\n",
        "          token = topi.squeeze(0).detach()  # detach from history as input\n",
        "\n",
        "          outputs.append(output)\n",
        "          if token == TEXT_EN.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "            \n",
        "        return torch.stack(outputs), torch.stack(att_list)\n",
        "\n",
        "\n",
        "    def teacher_forcing(self, hidden, encoder_outputs, target, lengths):\n",
        "\n",
        "        outputs = []\n",
        "        att_list = []\n",
        "        for i in range( target.size(0) ):\n",
        "          \n",
        "          # Build context vector with the attention layer\n",
        "          ### TODO\n",
        "          \n",
        "          # concatenate embedded representation with context vector\n",
        "          ### TODO\n",
        "          \n",
        "          # recurrent forward and inference\n",
        "          ### TODO\n",
        "\n",
        "        return torch.stack(outputs), torch.stack(att_list)\n",
        "      \n",
        "           \n",
        "\n",
        "vocab_size_en = len(TEXT_EN.vocab)\n",
        "hidden_size   = 128\n",
        "\n",
        "vocab_size_fr = len(TEXT_FR.vocab)\n",
        "max_length    = args['max_length']\n",
        "\n",
        "encoder = EncoderRNN(vocab_size_fr, hidden_size).to(args['device'])\n",
        "decoder = DecoderRNN(hidden_size, vocab_size_en, max_length).to(args['device'])\n",
        "\n",
        "print(encoder)\n",
        "print(decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbtjvr-PM9GM"
      },
      "source": [
        "# Setting optimizer.\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(),\n",
        "                       lr=args['lr'],\n",
        "                       weight_decay=args['weight_decay'],\n",
        "                       betas=(args['momentum'], 0.999))\n",
        "\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(),\n",
        "                       lr=args['lr'],\n",
        "                       weight_decay=args['weight_decay'],\n",
        "                       betas=(args['momentum'], 0.999))\n",
        "\n",
        "# Setting loss.\n",
        "criterion = nn.NLLLoss().to(args['device'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC7WBLNIUwFj"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy().squeeze(-1), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train(train_loader, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    teacher_forcing_p = 0.5\n",
        "\n",
        "    # Setting network for training mode.\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    train_loss = []\n",
        "    \n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(train_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        text, text_lengths = batch_data.text_fr\n",
        "        labs, labs_lengths = batch_data.text_en\n",
        "        \n",
        "        # Ignorando batches não ordenados para acelerar o treinamento\n",
        "        if sorted(labs_lengths, reverse=True) != list(labs_lengths.data):\n",
        "          continue\n",
        "\n",
        "        # Clears the gradients of optimizer.\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # Forwarding.\n",
        "        enc, enc_hidden = encoder(text, text_lengths.cpu())\n",
        "        if np.random.rand() > teacher_forcing_p:\n",
        "          outs, att = decoder(enc_hidden, enc)  \n",
        "          minlen = min(len(labs)-1, len(outs))\n",
        "        else:\n",
        "          outs, att = decoder(enc_hidden, enc, labs[:-1], labs_lengths.cpu()-1)\n",
        "          minlen = len(outs)\n",
        "\n",
        "        # Computing loss.\n",
        "        loss = 0.\n",
        "        for k in range(minlen):\n",
        "          loss += criterion(outs[k], labs[k+1])\n",
        "        loss = loss.mean()\n",
        "        \n",
        "        # Computing backpropagation.\n",
        "        loss.backward()\n",
        "        \n",
        "        # Weight update\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "        \n",
        "        # Updating lists.\n",
        "        train_loss.append(loss.data.item())\n",
        "\n",
        "        if (i+1) % 300 == 0:\n",
        "          input = [TEXT_FR.vocab.itos[t] for t in text[:,0]]\n",
        "          labels = [TEXT_EN.vocab.itos[t] for t in labs[:,0]]\n",
        "          output = [TEXT_EN.vocab.itos[np.argmax(t.cpu().data)] for t in outs[:,0]]\n",
        "          print(f'Input:{input}\\nLabel:{labels}\\nOutput:{output}',  )\n",
        "          \n",
        "          epoch_loss_ = np.asarray(train_loss)\n",
        "\n",
        "          print('--------------------------------------------------------------------')\n",
        "          print('[epoch %d iter %d], [train loss %.4f +/- %.4f], [epoch elapsed time %.2f]' % (\n",
        "              epoch, i+1, epoch_loss_.mean(), epoch_loss_.std(), (time.time() - tic)))\n",
        "          print('--------------------------------------------------------------------')\n",
        "\n",
        "          showAttention(input, output, att)\n",
        "    \n",
        "    toc = time.time()\n",
        "    \n",
        "    train_loss = np.asarray(train_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "    print('--------------------------------------------------------------------')\n",
        "    print('[epoch %d], [train loss %.4f +/- %.4f], [training time %.2f]' % (\n",
        "        epoch, train_loss.mean(), train_loss.std(), (toc - tic)))\n",
        "    print('--------------------------------------------------------------------')\n",
        "\n",
        "def test(test_loader, criterion, epoch):\n",
        "\n",
        "    tic = time.time()\n",
        "    \n",
        "    # Setting network for evaluation mode (not computing gradients).\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    # Lists for losses and metrics.\n",
        "    test_loss = []\n",
        "    \n",
        "    print('********************************************************************')\n",
        "    # Iterating over batches.\n",
        "    for i, batch_data in enumerate(test_loader):\n",
        "\n",
        "        # Obtaining images, labels and paths for batch.\n",
        "        text, text_lengths = batch_data.text_fr\n",
        "        labs, labs_lengths = batch_data.text_en\n",
        "        \n",
        "        \n",
        "        # Forwarding.\n",
        "        enc, enc_hidden  = encoder(text, text_lengths.cpu())\n",
        "        outs, att = decoder(enc_hidden, enc)\n",
        "        \n",
        "        if i < 2:\n",
        "          input = [TEXT_FR.vocab.itos[t] for t in text[:,0]]\n",
        "          labels = [TEXT_EN.vocab.itos[t] for t in labs[:,0]]\n",
        "          output = [TEXT_EN.vocab.itos[np.argmax(t.cpu().data)] for t in outs[:,0]]\n",
        "          print(f'Input:{input}\\nLabel:{labels}\\nOutput:{output}',  )\n",
        "\n",
        "          showAttention(input, output, att)\n",
        "        \n",
        "        # Computing approximate loss \n",
        "        labs = labs[1:]\n",
        "        minlen = min(len(labs), len(outs))\n",
        "        \n",
        "        loss = 0.\n",
        "        for k in range(minlen):\n",
        "          loss += criterion(outs[k], labs[k])\n",
        "        loss = loss.mean()\n",
        "                \n",
        "        # Updating lists.\n",
        "        test_loss.append(loss.data.item())\n",
        "    \n",
        "    toc = time.time()\n",
        "\n",
        "    test_loss = np.asarray(test_loss)\n",
        "    \n",
        "    # Printing training epoch loss and metrics.\n",
        "   \n",
        "    print('[epoch %d], [test loss %.4f +/- %.4f], [testing time %.2f]' % (\n",
        "        epoch, test_loss.mean(), test_loss.std(), (toc - tic)))\n",
        "    print('********************************************************************')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7scMkTCQhADF"
      },
      "source": [
        "# Iterating over epochs.\n",
        "for epoch in range(1, args['epoch_num'] + 1):\n",
        "\n",
        "    # Training function.\n",
        "    train(train_iterator, criterion, epoch)\n",
        "\n",
        "    # Computing test loss and metrics.\n",
        "    test(test_iterator, criterion, epoch)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}